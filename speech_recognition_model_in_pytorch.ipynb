{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech Recognition model in PyTorch",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwfN8o17Bdp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ccd7d3b-a5e5-4e46-a9e6-be1a1492fd9e"
      },
      "source": [
        "!pip install torchaudio==0.4.0 torch==1.4.0 python-Levenshtein==0.12.1 jiwer==2.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/bc/3ebc127162d27bed33dc914606f10117d106680baae7ce83603ea09985fd/torchaudio-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 6.0MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n",
            "\u001b[?25hCollecting python-Levenshtein==0.12.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/ca/1a9d7115f233d929d4f25a4021795cd97cc89eeb82723ea98dd44390a530/python-Levenshtein-0.12.1.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25hCollecting jiwer==2.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/cc/fb9d3132cba1f6d393b7d5a9398d9d4c8fc033bc54668cf87e9b197a6d7a/jiwer-2.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein==0.12.1) (51.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from jiwer==2.2.0) (1.19.5)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.1-cp36-cp36m-linux_x86_64.whl size=149167 sha256=5ceba1720861a4efeb233c6274e3932a68dc9faf2fdccd6dfe4f2443707ceb91\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/69/ea/4798f98317cbab35d78fea64d36bd7b2b18faca88568ef15b0\n",
            "Successfully built python-Levenshtein\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchaudio, python-Levenshtein, jiwer\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed jiwer-2.2.0 python-Levenshtein-0.12.1 torch-1.4.0 torchaudio-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSKHvy8DmOCQ"
      },
      "source": [
        "## Setting up your data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5EfeiToVBRh"
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "import string\n",
        "from typing import List, Dict\n",
        "\n",
        "import Levenshtein\n",
        "import jiwer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchaudio\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVJs4Bk8FjjO"
      },
      "source": [
        "def avg_wer(wer_scores: List[float], combined_ref_len: int) -> float:\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "\n",
        "def wer(reference: str, hypothesis: str) -> float:\n",
        "    ref_words = reference.split(' ')\n",
        "    ref_len = len(ref_words)\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "    hyp_words = hypothesis.split(' ')\n",
        "    word_edit = 0\n",
        "    for ref_word, hyp_word in itertools.zip_longest(ref_words, hyp_words, fillvalue=' '):\n",
        "        distance = Levenshtein.distance(ref_word, hyp_word)\n",
        "        word_edit += 0 if distance == 0 else 1\n",
        "\n",
        "    return float(word_edit) / ref_len\n",
        "\n",
        "\n",
        "def cer(reference: str, hypothesis: str):\n",
        "    ref_len = len(reference)\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "    join_char = ' '\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "    edit_distance = Levenshtein.distance(reference, hypothesis)\n",
        "\n",
        "    return float(edit_distance) / ref_len\n",
        "\n",
        "\n",
        "class TextTransform:\n",
        "    def __init__(self):\n",
        "        self.char_map: Dict[str, int] = {}\n",
        "        self.index_map: Dict[int, str] = {}\n",
        "        alphabets = list(string.ascii_lowercase)\n",
        "        alphabets.insert(0, ' ')\n",
        "        alphabets.insert(1, '<SPACE>')\n",
        "        for idx, alpha in enumerate(alphabets):\n",
        "            self.char_map[alpha] = idx\n",
        "            self.index_map[idx] = alpha\n",
        "        self.preprocess = jiwer.Compose([\n",
        "            jiwer.ToLowerCase(),\n",
        "            jiwer.ExpandCommonEnglishContractions(),\n",
        "            jiwer.RemovePunctuation(),\n",
        "            jiwer.RemoveMultipleSpaces()\n",
        "        ])\n",
        "\n",
        "    def text_to_int(self, text: str):\n",
        "        text = self.preprocess(text)\n",
        "        int_sequence = []\n",
        "        for char in text:\n",
        "            char = '<SPACE>' if char == ' ' else char\n",
        "            int_sequence.append(self.char_map[char])\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels: List[int]):\n",
        "        str_label = []\n",
        "        for int_repr in labels:\n",
        "            str_label.append(self.index_map[int_repr])\n",
        "        return ''.join(str_label).replace('<SPACE>', ' ')\n",
        "\n",
        "\n",
        "def _train_audio_transforms(sample_rate: int):\n",
        "    return nn.Sequential(\n",
        "        torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate),\n",
        "        torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "        torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        "    )\n",
        "\n",
        "\n",
        "def _valid_audio_transforms(sample_rate: int):\n",
        "    return torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)\n",
        "\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "\n",
        "def preprocessing(data, data_type=\"train\") -> (List[Tensor], List[Tensor], List[int], List[int], List[str]):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, sample_rate, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spectrogram = _train_audio_transforms(sample_rate=sample_rate)(waveform)\n",
        "        else:\n",
        "            spectrogram = _valid_audio_transforms(sample_rate=sample_rate)(waveform)\n",
        "        spectrogram = spectrogram.squeeze(0).transpose(0, 1)\n",
        "        spectrograms.append(spectrogram)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spectrogram.shape[0] // 2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def greedy_decoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j - 1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(text_transform.int_to_text(decode))\n",
        "    return decodes, targets\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XdSlhAQnDEA"
      },
      "source": [
        "## The Model\n",
        "Base of of Deep Speech 2 with some personal improvements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65H1-PCjm-FB"
      },
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel // 2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel // 2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x  # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats // 2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=1)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats * 32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i == 0 else rnn_dim * 2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i == 0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim * 2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuguNEzKnMOn"
      },
      "source": [
        "## The Training and Evaluating Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydkqGeOwnPGY"
      },
      "source": [
        "def train(model: SpeechRecognitionModel, device, train_loader: DataLoader, criterion: nn.CTCLoss, optimizer,\n",
        "          scheduler, epoch):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(spectrograms)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1)  # (time, batch, n_class)\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(spectrograms), data_len,\n",
        "                       100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model: SpeechRecognitionModel, device, test_loader: DataLoader, criterion: nn.CTCLoss):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1)  # (time, batch, n_class)\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "            decoded_preds, decoded_targets = greedy_decoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                predictions.append(f\"Prediction is: {decoded_preds[j]} | actual is: {decoded_targets[j]}.\\n\")\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "    avg_cer = sum(test_cer) / len(test_cer)\n",
        "    avg_wer = sum(test_wer) / len(test_wer)\n",
        "    with open(\"./data/experiment.txt\", \"w\") as f:\n",
        "        f.writelines(predictions)\n",
        "    print(\n",
        "        'Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxRIb_WempDq"
      },
      "source": [
        "## GPU runtime\n",
        "If you are using a GPU runtime, this will let you know what GPU and how much memory is available. Adjust your batch_size depending on which GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlUSuAJwlzo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497af1f5-3404-45af-b25f-a98327fe3688"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jan 25 08:40:30 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXvlWZeVpXfX"
      },
      "source": [
        "## Train\n",
        "this will download the data on first run and may take a while. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucfQX3qN21az",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b3b65d4-126e-4e89-add0-2d45227cf9d2"
      },
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 20\n",
        "epochs = 20\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "hparams = {\n",
        "    \"n_cnn_layers\": 1,\n",
        "    \"n_rnn_layers\": 1,\n",
        "    \"rnn_dim\": 512,\n",
        "    \"n_class\": 29,\n",
        "    \"n_feats\": 128,\n",
        "    \"stride\": 2,\n",
        "    \"dropout\": 0.1,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"epochs\": epochs\n",
        "}\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(7)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "if not os.path.isdir(\"./data\"):\n",
        "    os.makedirs(\"./data\")\n",
        "\n",
        "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=libri_train_set, download=True)\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=libri_test_set, download=True)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                               batch_size=hparams['batch_size'],\n",
        "                               shuffle=False,\n",
        "                               collate_fn=lambda x: preprocessing(x, 'train'),\n",
        "                               **kwargs)\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=hparams['batch_size'],\n",
        "                              shuffle=False,\n",
        "                              collate_fn=lambda x: preprocessing(x, 'valid'),\n",
        "                              **kwargs)\n",
        "\n",
        "speech_model = SpeechRecognitionModel(\n",
        "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        ").to(device)\n",
        "\n",
        "print(speech_model)\n",
        "print('Num Model Parameters', sum([param.nelement() for param in speech_model.parameters()]))\n",
        "\n",
        "optimizer = optim.AdamW(speech_model.parameters(), hparams['learning_rate'])\n",
        "criterion = nn.CTCLoss(blank=28).to(device)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
        "                                          steps_per_epoch=int(len(train_loader)),\n",
        "                                          epochs=hparams['epochs'],\n",
        "                                          anneal_strategy='linear')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(speech_model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
        "    test(speech_model, device, test_loader, criterion)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SpeechRecognitionModel(\n",
            "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (rescnn_layers): Sequential(\n",
            "    (0): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (birnn_layers): Sequential(\n",
            "    (0): BidirectionalGRU(\n",
            "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): GELU()\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
            "  )\n",
            ")\n",
            "Num Model Parameters 4760733\n",
            "Train Epoch: 1 [0/28539 (0%)]\tLoss: 7.286835\n",
            "Train Epoch: 1 [2000/28539 (7%)]\tLoss: 2.932901\n",
            "Train Epoch: 1 [4000/28539 (14%)]\tLoss: 2.989086\n",
            "Train Epoch: 1 [6000/28539 (21%)]\tLoss: 2.861118\n",
            "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 2.836002\n",
            "Train Epoch: 1 [10000/28539 (35%)]\tLoss: 3.062391\n",
            "Train Epoch: 1 [12000/28539 (42%)]\tLoss: 2.860567\n",
            "Train Epoch: 1 [14000/28539 (49%)]\tLoss: 2.844348\n",
            "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 2.876820\n",
            "Train Epoch: 1 [18000/28539 (63%)]\tLoss: 2.898701\n",
            "Train Epoch: 1 [20000/28539 (70%)]\tLoss: 2.916926\n",
            "Train Epoch: 1 [22000/28539 (77%)]\tLoss: 2.867815\n",
            "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 2.770080\n",
            "Train Epoch: 1 [26000/28539 (91%)]\tLoss: 2.790650\n",
            "Train Epoch: 1 [28000/28539 (98%)]\tLoss: 2.640002\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.6480, Average CER: 0.853072 Average WER: 1.0611\n",
            "\n",
            "Train Epoch: 2 [0/28539 (0%)]\tLoss: 2.682114\n",
            "Train Epoch: 2 [2000/28539 (7%)]\tLoss: 2.520308\n",
            "Train Epoch: 2 [4000/28539 (14%)]\tLoss: 2.425285\n",
            "Train Epoch: 2 [6000/28539 (21%)]\tLoss: 2.367787\n",
            "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 2.228074\n",
            "Train Epoch: 2 [10000/28539 (35%)]\tLoss: 2.375062\n",
            "Train Epoch: 2 [12000/28539 (42%)]\tLoss: 2.335417\n",
            "Train Epoch: 2 [14000/28539 (49%)]\tLoss: 2.248194\n",
            "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 2.168638\n",
            "Train Epoch: 2 [18000/28539 (63%)]\tLoss: 2.134286\n",
            "Train Epoch: 2 [20000/28539 (70%)]\tLoss: 2.187504\n",
            "Train Epoch: 2 [22000/28539 (77%)]\tLoss: 2.175650\n",
            "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 1.919570\n",
            "Train Epoch: 2 [26000/28539 (91%)]\tLoss: 2.149477\n",
            "Train Epoch: 2 [28000/28539 (98%)]\tLoss: 2.029200\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.8688, Average CER: 0.614606 Average WER: 1.5831\n",
            "\n",
            "Train Epoch: 3 [0/28539 (0%)]\tLoss: 2.019253\n",
            "Train Epoch: 3 [2000/28539 (7%)]\tLoss: 1.903038\n",
            "Train Epoch: 3 [4000/28539 (14%)]\tLoss: 1.832304\n",
            "Train Epoch: 3 [6000/28539 (21%)]\tLoss: 1.898211\n",
            "Train Epoch: 3 [8000/28539 (28%)]\tLoss: 1.732475\n",
            "Train Epoch: 3 [10000/28539 (35%)]\tLoss: 1.930357\n",
            "Train Epoch: 3 [12000/28539 (42%)]\tLoss: 1.968687\n",
            "Train Epoch: 3 [14000/28539 (49%)]\tLoss: 1.826337\n",
            "Train Epoch: 3 [16000/28539 (56%)]\tLoss: 1.800509\n",
            "Train Epoch: 3 [18000/28539 (63%)]\tLoss: 1.744728\n",
            "Train Epoch: 3 [20000/28539 (70%)]\tLoss: 1.841823\n",
            "Train Epoch: 3 [22000/28539 (77%)]\tLoss: 1.841456\n",
            "Train Epoch: 3 [24000/28539 (84%)]\tLoss: 1.539296\n",
            "Train Epoch: 3 [26000/28539 (91%)]\tLoss: 1.894217\n",
            "Train Epoch: 3 [28000/28539 (98%)]\tLoss: 1.661736\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.5400, Average CER: 0.451719 Average WER: 1.1251\n",
            "\n",
            "Train Epoch: 4 [0/28539 (0%)]\tLoss: 1.704004\n",
            "Train Epoch: 4 [2000/28539 (7%)]\tLoss: 1.609176\n",
            "Train Epoch: 4 [4000/28539 (14%)]\tLoss: 1.543355\n",
            "Train Epoch: 4 [6000/28539 (21%)]\tLoss: 1.716106\n",
            "Train Epoch: 4 [8000/28539 (28%)]\tLoss: 1.475170\n",
            "Train Epoch: 4 [10000/28539 (35%)]\tLoss: 1.684216\n",
            "Train Epoch: 4 [12000/28539 (42%)]\tLoss: 1.654963\n",
            "Train Epoch: 4 [14000/28539 (49%)]\tLoss: 1.678482\n",
            "Train Epoch: 4 [16000/28539 (56%)]\tLoss: 1.573938\n",
            "Train Epoch: 4 [18000/28539 (63%)]\tLoss: 1.553211\n",
            "Train Epoch: 4 [20000/28539 (70%)]\tLoss: 1.626786\n",
            "Train Epoch: 4 [22000/28539 (77%)]\tLoss: 1.699174\n",
            "Train Epoch: 4 [24000/28539 (84%)]\tLoss: 1.332454\n",
            "Train Epoch: 4 [26000/28539 (91%)]\tLoss: 1.722343\n",
            "Train Epoch: 4 [28000/28539 (98%)]\tLoss: 1.418539\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.3186, Average CER: 0.395005 Average WER: 0.9997\n",
            "\n",
            "Train Epoch: 5 [0/28539 (0%)]\tLoss: 1.580381\n",
            "Train Epoch: 5 [2000/28539 (7%)]\tLoss: 1.431314\n",
            "Train Epoch: 5 [4000/28539 (14%)]\tLoss: 1.328002\n",
            "Train Epoch: 5 [6000/28539 (21%)]\tLoss: 1.535939\n",
            "Train Epoch: 5 [8000/28539 (28%)]\tLoss: 1.274546\n",
            "Train Epoch: 5 [10000/28539 (35%)]\tLoss: 1.539902\n",
            "Train Epoch: 5 [12000/28539 (42%)]\tLoss: 1.553376\n",
            "Train Epoch: 5 [14000/28539 (49%)]\tLoss: 1.449950\n",
            "Train Epoch: 5 [16000/28539 (56%)]\tLoss: 1.378627\n",
            "Train Epoch: 5 [18000/28539 (63%)]\tLoss: 1.482961\n",
            "Train Epoch: 5 [20000/28539 (70%)]\tLoss: 1.439128\n",
            "Train Epoch: 5 [22000/28539 (77%)]\tLoss: 1.529717\n",
            "Train Epoch: 5 [24000/28539 (84%)]\tLoss: 1.119544\n",
            "Train Epoch: 5 [26000/28539 (91%)]\tLoss: 1.527835\n",
            "Train Epoch: 5 [28000/28539 (98%)]\tLoss: 1.341580\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.1999, Average CER: 0.360927 Average WER: 0.9562\n",
            "\n",
            "Train Epoch: 6 [0/28539 (0%)]\tLoss: 1.448123\n",
            "Train Epoch: 6 [2000/28539 (7%)]\tLoss: 1.327438\n",
            "Train Epoch: 6 [4000/28539 (14%)]\tLoss: 1.181248\n",
            "Train Epoch: 6 [6000/28539 (21%)]\tLoss: 1.323950\n",
            "Train Epoch: 6 [8000/28539 (28%)]\tLoss: 1.193659\n",
            "Train Epoch: 6 [10000/28539 (35%)]\tLoss: 1.481130\n",
            "Train Epoch: 6 [12000/28539 (42%)]\tLoss: 1.483511\n",
            "Train Epoch: 6 [14000/28539 (49%)]\tLoss: 1.421654\n",
            "Train Epoch: 6 [16000/28539 (56%)]\tLoss: 1.312551\n",
            "Train Epoch: 6 [18000/28539 (63%)]\tLoss: 1.315225\n",
            "Train Epoch: 6 [20000/28539 (70%)]\tLoss: 1.334507\n",
            "Train Epoch: 6 [22000/28539 (77%)]\tLoss: 1.432787\n",
            "Train Epoch: 6 [24000/28539 (84%)]\tLoss: 1.036706\n",
            "Train Epoch: 6 [26000/28539 (91%)]\tLoss: 1.475586\n",
            "Train Epoch: 6 [28000/28539 (98%)]\tLoss: 1.480996\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.1665, Average CER: 0.348390 Average WER: 0.9408\n",
            "\n",
            "Train Epoch: 7 [0/28539 (0%)]\tLoss: 1.422524\n",
            "Train Epoch: 7 [2000/28539 (7%)]\tLoss: 1.215384\n",
            "Train Epoch: 7 [4000/28539 (14%)]\tLoss: 1.179209\n",
            "Train Epoch: 7 [6000/28539 (21%)]\tLoss: 1.360958\n",
            "Train Epoch: 7 [8000/28539 (28%)]\tLoss: 1.174935\n",
            "Train Epoch: 7 [10000/28539 (35%)]\tLoss: 1.381966\n",
            "Train Epoch: 7 [12000/28539 (42%)]\tLoss: 1.347669\n",
            "Train Epoch: 7 [14000/28539 (49%)]\tLoss: 1.246043\n",
            "Train Epoch: 7 [16000/28539 (56%)]\tLoss: 1.242266\n",
            "Train Epoch: 7 [18000/28539 (63%)]\tLoss: 1.204751\n",
            "Train Epoch: 7 [20000/28539 (70%)]\tLoss: 1.356709\n",
            "Train Epoch: 7 [22000/28539 (77%)]\tLoss: 1.353541\n",
            "Train Epoch: 7 [24000/28539 (84%)]\tLoss: 0.925850\n",
            "Train Epoch: 7 [26000/28539 (91%)]\tLoss: 1.342508\n",
            "Train Epoch: 7 [28000/28539 (98%)]\tLoss: 1.189085\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.0108, Average CER: 0.305454 Average WER: 0.9132\n",
            "\n",
            "Train Epoch: 8 [0/28539 (0%)]\tLoss: 1.266346\n",
            "Train Epoch: 8 [2000/28539 (7%)]\tLoss: 1.112934\n",
            "Train Epoch: 8 [4000/28539 (14%)]\tLoss: 1.070234\n",
            "Train Epoch: 8 [6000/28539 (21%)]\tLoss: 1.287034\n",
            "Train Epoch: 8 [8000/28539 (28%)]\tLoss: 1.022048\n",
            "Train Epoch: 8 [10000/28539 (35%)]\tLoss: 1.317961\n",
            "Train Epoch: 8 [12000/28539 (42%)]\tLoss: 1.304479\n",
            "Train Epoch: 8 [14000/28539 (49%)]\tLoss: 1.251900\n",
            "Train Epoch: 8 [16000/28539 (56%)]\tLoss: 1.133006\n",
            "Train Epoch: 8 [18000/28539 (63%)]\tLoss: 1.112609\n",
            "Train Epoch: 8 [20000/28539 (70%)]\tLoss: 1.220227\n",
            "Train Epoch: 8 [22000/28539 (77%)]\tLoss: 1.308291\n",
            "Train Epoch: 8 [24000/28539 (84%)]\tLoss: 0.892246\n",
            "Train Epoch: 8 [26000/28539 (91%)]\tLoss: 1.363103\n",
            "Train Epoch: 8 [28000/28539 (98%)]\tLoss: 1.092590\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.9296, Average CER: 0.279312 Average WER: 0.8930\n",
            "\n",
            "Train Epoch: 9 [0/28539 (0%)]\tLoss: 1.106690\n",
            "Train Epoch: 9 [2000/28539 (7%)]\tLoss: 0.994640\n",
            "Train Epoch: 9 [4000/28539 (14%)]\tLoss: 0.968798\n",
            "Train Epoch: 9 [6000/28539 (21%)]\tLoss: 1.163222\n",
            "Train Epoch: 9 [8000/28539 (28%)]\tLoss: 0.981120\n",
            "Train Epoch: 9 [10000/28539 (35%)]\tLoss: 1.216165\n",
            "Train Epoch: 9 [12000/28539 (42%)]\tLoss: 1.218623\n",
            "Train Epoch: 9 [14000/28539 (49%)]\tLoss: 1.182055\n",
            "Train Epoch: 9 [16000/28539 (56%)]\tLoss: 1.141747\n",
            "Train Epoch: 9 [18000/28539 (63%)]\tLoss: 1.082427\n",
            "Train Epoch: 9 [20000/28539 (70%)]\tLoss: 1.119246\n",
            "Train Epoch: 9 [22000/28539 (77%)]\tLoss: 1.237787\n",
            "Train Epoch: 9 [24000/28539 (84%)]\tLoss: 0.837305\n",
            "Train Epoch: 9 [26000/28539 (91%)]\tLoss: 1.150727\n",
            "Train Epoch: 9 [28000/28539 (98%)]\tLoss: 1.035406\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.8886, Average CER: 0.268320 Average WER: 0.8891\n",
            "\n",
            "Train Epoch: 10 [0/28539 (0%)]\tLoss: 1.149898\n",
            "Train Epoch: 10 [2000/28539 (7%)]\tLoss: 1.037716\n",
            "Train Epoch: 10 [4000/28539 (14%)]\tLoss: 0.957806\n",
            "Train Epoch: 10 [6000/28539 (21%)]\tLoss: 1.139274\n",
            "Train Epoch: 10 [8000/28539 (28%)]\tLoss: 0.907180\n",
            "Train Epoch: 10 [10000/28539 (35%)]\tLoss: 1.205884\n",
            "Train Epoch: 10 [12000/28539 (42%)]\tLoss: 1.174210\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}